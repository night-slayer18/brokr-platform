# Performance Analysis Report

This document provides a comprehensive analysis of the Brokr Platform codebase, identifying existing performance optimizations and suggesting further improvements.

## Executive Summary

The Brokr Platform codebase already implements many performance best practices. This analysis identifies the existing optimizations and suggests additional improvements that could further enhance performance.

## Existing Performance Optimizations âœ…

### 1. Backend - Batch Operations (Already Optimized)

#### KafkaAdminService
The `KafkaAdminService` has been optimized to use batch operations instead of individual queries:

- **`getTopicsBatch()`** - Fetches detailed information for multiple topics in 3 Kafka API calls instead of 3N calls
- **`getConsumerGroupOffsetsBatch()`** - Batch fetches offsets for all consumer groups efficiently
- **`listConsumerGroups()`** - Uses batch `describeConsumerGroups()` call instead of N individual calls

```java
// Example: 1000 topics reduced from 3000+ calls to just 3 calls
public Map<String, Topic> getTopicsBatch(KafkaCluster cluster, List<String> topicNames)
```

#### MetricsCollectionService
- Uses parallel processing with `CompletableFuture` for cluster metrics collection
- Batches topic and consumer group metrics saves
- Implements caching via `TopicMetricsCache` for throughput calculations

### 2. GraphQL Resolvers (Already Optimized)

#### ClusterResolver - @BatchMapping
The `ClusterResolver` uses GraphQL's `@BatchMapping` to prevent N+1 queries:

```java
@BatchMapping(typeName = "KafkaCluster", field = "organization")
public Map<KafkaCluster, Organization> getOrganization(List<KafkaCluster> clusters) {
    // Batch fetch all organizations in ONE query
    List<String> organizationIds = clusters.stream().map(KafkaCluster::getOrganizationId).distinct().toList();
    Map<String, Organization> orgsById = organizationApiService.getOrganizationsByIds(organizationIds);
    return clusters.stream().collect(Collectors.toMap(Function.identity(), c -> orgsById.get(c.getOrganizationId())));
}
```

Similar batch mappings exist for:
- `environment`
- `schemaRegistries`
- `kafkaConnects`
- `kafkaStreamsApplications`
- `ksqlDBs`
- `brokers`
- `topics`
- `consumerGroups`

### 3. Database Query Optimizations

#### TopicMetricsRepository
- Optimized queries with proper indexes
- Uses pagination for large result sets
- Implements batch delete operations

#### ApiKeyUsageService
- Uses bounded queue for async batch inserts
- Prevents memory exhaustion with configurable queue capacity
- Processes records in batches (default: 100 records)

### 4. Pagination Support

- **Topics**: `listTopicsPaginated()` fetches topic names first, then details only for the current page
- **Users**: `UserResolver` supports pagination
- **Audit Logs**: `AuditLogResolver` supports pagination with filtering

### 5. Caching Implementations

- **TopicMetricsCache**: Caches previous metrics for throughput calculations
- **KafkaConnectionService**: Caches AdminClient connections

### 6. Async Processing

- **MetricsCollectionService**: Uses `@Async("metricsCollectionExecutor")` for non-blocking metrics collection
- **ApiKeyUsageService**: Records usage asynchronously with `@Async`

---

## Suggested Further Improvements ðŸ”§

### 1. Frontend Performance

#### Issue: Message Table Re-renders
**Location**: `brokr-frontend/src/pages/TopicDetailPage.tsx`

The message table creates a new function `truncateText` on every render:

```typescript
// Current - creates function on every render
{paginatedMessages.map((message: Message, index: number) => {
    const truncateText = (text: string | null | undefined, maxLength: number = 50) => {
        // ...
    };
```

**Suggested Fix**: Move `truncateText` outside the component or memoize it:

```typescript
// Optimized - defined once
const truncateText = (text: string | null | undefined, maxLength: number = 50) => {
    if (!text) return null;
    if (text.length <= maxLength) return text;
    return `${text.substring(0, maxLength)}...`;
};

function TopicDetailPage() {
    // ... use truncateText
}
```

#### Issue: Missing React.memo for Child Components
**Location**: `brokr-frontend/src/pages/ClusterOverviewPage.tsx`

The `StatCard` component is correctly memoized, but the `ClusterMetricsChart` and `TopicMetricsChart` could benefit from similar optimization if they aren't already memoized.

### 2. Backend Database Indexes

#### Suggestion: Add Composite Index for Audit Logs
**Location**: `brokr-backend/brokr-storage/src/main/java/io/brokr/storage/entity/AuditLogEntity.java`

The `findWithFilters` query could benefit from additional composite indexes:

```sql
-- Suggested indexes for common query patterns
CREATE INDEX idx_audit_logs_org_timestamp ON audit_logs(organization_id, timestamp DESC);
CREATE INDEX idx_audit_logs_user_timestamp ON audit_logs(user_id, timestamp DESC);
CREATE INDEX idx_audit_logs_action_status_timestamp ON audit_logs(action_type, status, timestamp DESC);
```

### 3. Consumer Group Offset Fetching

#### Issue: Parallel offset fetching could be more efficient
**Location**: `brokr-backend/brokr-kafka/src/main/java/io/brokr/kafka/service/KafkaAdminService.java`

The `getConsumerGroupOffsetsBatch()` method uses `CompletableFuture.supplyAsync()` without a custom executor:

```java
// Current
List<CompletableFuture<...>> groupOffsetFutures = groupIds.stream()
    .map(groupId -> CompletableFuture.supplyAsync(() -> { ... }))
    .collect(Collectors.toList());
```

**Suggested Fix**: Use a dedicated executor for Kafka operations to control thread pool size:

```java
// Optimized - with dedicated executor
private final Executor kafkaOperationsExecutor;

List<CompletableFuture<...>> groupOffsetFutures = groupIds.stream()
    .map(groupId -> CompletableFuture.supplyAsync(() -> { ... }, kafkaOperationsExecutor))
    .collect(Collectors.toList());
```

### 4. API Key Usage Statistics

#### Issue: Multiple database queries for statistics
**Location**: `brokr-backend/brokr-security/src/main/java/io/brokr/security/service/ApiKeyUsageService.java`

The `getUsageStatistics()` method executes 5 separate database queries:

```java
long totalRequests = usageRepository.countByApiKeyIdAndCreatedAtBetween(...);
long errorCount = usageRepository.countErrors(...);
Double avgResponseTime = usageRepository.getAverageResponseTime(...);
List<Object[]> statusCodeCounts = usageRepository.countByApiKeyIdAndStatusCode(...);
List<Object[]> timeSeriesData = usageRepository.countByApiKeyIdGroupedByHour(...);
```

**Suggested Fix**: Consider combining some queries using a single aggregation query or running queries in parallel:

```java
// Run independent queries in parallel
CompletableFuture<Long> totalFuture = CompletableFuture.supplyAsync(() -> 
    usageRepository.countByApiKeyIdAndCreatedAtBetween(...));
CompletableFuture<Long> errorFuture = CompletableFuture.supplyAsync(() -> 
    usageRepository.countErrors(...));
// ... etc

CompletableFuture.allOf(totalFuture, errorFuture, ...).join();
```

### 5. Retry Logic with Circuit Breaker

#### Suggestion: Add Circuit Breaker Pattern
**Location**: `brokr-backend/brokr-kafka/src/main/java/io/brokr/kafka/service/KafkaAdminService.java`

The current `@Retryable` annotations handle transient failures, but adding a circuit breaker would prevent cascading failures:

```java
// Suggested - add Resilience4j CircuitBreaker
@CircuitBreaker(name = "kafkaAdmin", fallbackMethod = "listTopicsFallback")
@Retryable(...)
public List<Topic> listTopics(KafkaCluster cluster) { ... }
```

### 6. Message Replay Streaming

#### Observation: Well-optimized but could benefit from backpressure
**Location**: `brokr-backend/brokr-kafka/src/main/java/io/brokr/kafka/service/KafkaConsumerService.java`

The `streamMessagesForReplay()` method is well-optimized with batch processing. Consider adding backpressure handling if the message processor is slower than consumption:

```java
// Suggestion: Add backpressure with bounded queue
BlockingQueue<List<Message>> processingQueue = new ArrayBlockingQueue<>(10);

// Consumer thread adds to queue (blocks if full)
processingQueue.put(batch);

// Processor thread takes from queue
List<Message> toProcess = processingQueue.take();
```

---

## Performance Metrics to Monitor

1. **Kafka Operations**:
   - Average time for `listTopics()`, `getTopicsBatch()`
   - Consumer group offset fetch latency

2. **GraphQL Queries**:
   - Query execution time per resolver
   - DataLoader batch sizes

3. **Database**:
   - Query execution times
   - Connection pool utilization

4. **Memory**:
   - Heap usage during metrics collection
   - Message replay memory consumption

---

## Conclusion

The Brokr Platform codebase demonstrates mature performance optimization practices, particularly in:

1. **Batch operations** to eliminate N+1 problems
2. **GraphQL @BatchMapping** for efficient data fetching
3. **Async processing** for non-blocking operations
4. **Pagination** for large data sets
5. **Caching** for frequently accessed data

The suggested improvements are refinements that could provide incremental performance gains, but the core architecture is already well-optimized for enterprise-scale Kafka management.
